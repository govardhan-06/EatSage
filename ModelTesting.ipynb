{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFF9DXYgLAQectfCTmtzFy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/govardhan-06/EatSage/blob/devBackend/ModelTesting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIJykJe-mUYL",
        "outputId": "6079b404-82d3-4120-8deb-1dba9b877de4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.2/374.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet  langchain-google-genai pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langchain_core.prompts import HumanMessagePromptTemplate, ChatPromptTemplate"
      ],
      "metadata": {
        "id": "wFaRDwtdmaBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contextLoader():\n",
        "    \"\"\"This function loads the context of the current environment and returns it as a list of strings.\"\"\"\n",
        "    context = []\n",
        "    with open(\"/content/llmContext.txt\",'r') as f:\n",
        "        context=f.readlines()\n",
        "\n",
        "    return context"
      ],
      "metadata": {
        "id": "eANYrEhzmcMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GEMINI_API_KEY=\"AIzaSyC8g1rLwa2d9wt6cUYRM3ilACmPdLmqjZM\"\n",
        "context=contextLoader()\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",convert_system_message_to_human=True,api_key=GEMINI_API_KEY)\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=(\n",
        "                \"You are a friendly health assistant, who helps users to find the perfect food items based on their specific needs and preferences.\"\n",
        "                  \"You must suggest delicious and nutritious options to keep their feeling to the best\"\n",
        "                  \"Data to be returned : Food item, Restaurant, Price\"\n",
        "                  f\"Use this context to suggest the food items: {context}\"\n",
        "            )\n",
        "        ),\n",
        "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
        "    ]\n",
        ")\n",
        "chain=chat_template | llm\n",
        "llmOutput=chain.invoke({\"text\":\"Hey I prefer some italian cuisines, suggest me something good\"})"
      ],
      "metadata": {
        "id": "dmd0UBuomf_5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9488079-da44-402f-8ba0-8a5d487d5d88"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py:350: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llmOutput)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXyjoc99mhnS",
        "outputId": "3f3b5bfb-2ecb-47b6-a1fc-8f6cb7f4364e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='1. Spaghetti Carbonara from Bistro Bliss (Rs. 454)\\n2. Margherita Pizza from Taste Paradise (Rs. 296)\\n3. Spaghetti Carbonara from Gastronomy Grove (Rs. 521)\\n4. Margherita Pizza from Savory Delights (Rs. 296)\\n5. Spaghetti Carbonara from Savory Delights (Rs. 521)\\n6. Margherita Pizza from Flavor Fusion (Rs. 490)\\n7. Spaghetti Carbonara from Palate Pleasers (Rs. 521)\\n8. Margherita Pizza from Gastronomy Grove (Rs. 296)\\n9. Spaghetti Carbonara from Culinary Cafe (Rs. 521)' response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-5be0c41a-8ea4-4125-8347-2e4f294f1147-0' usage_metadata={'input_tokens': 16000, 'output_tokens': 156, 'total_tokens': 16156}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Issue with the gemini-pro model: reliability"
      ],
      "metadata": {
        "id": "5jULdjX9p-Ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bZYsSLjzAmk",
        "outputId": "c08f56e2-9a89-407f-e1c0-dee92e61607e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/103.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/103.5 kB\u001b[0m \u001b[31m764.6 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/103.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq"
      ],
      "metadata": {
        "id": "AKpyMi2_y77N"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GROQ_API_KEY=\"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\"\n",
        "context=contextLoader()\n",
        "llm=ChatGroq(temperature=0,model=\"llama3-70b-8192\",api_key=GROQ_API_KEY)\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=(\n",
        "                \"You are a friendly health assistant, who helps users to find the perfect food items based on their specific needs and preferences.\"\n",
        "                  \"You must suggest delicious and nutritious options to keep their feeling to the best\"\n",
        "                  \"Also, try to club the food suggestions that are from a single restaurant. Pick out the best options rather than suggesting food items from all the restaurants\"\n",
        "                  \"Data for a single restaurant must be returned must be in this format. Follow repeat this format, for all the restaurants\"\n",
        "                  '{\"Restaurant\" : <value>, \"Dishes\" :[\"itemname\": <value>,\"description\": <value>,\"itemcost\": <value>]}'\n",
        "                  f\"Use this context to suggest the food items: {context}\"\n",
        "            )\n",
        "        ),\n",
        "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
        "    ]\n",
        ")\n",
        "chain=chat_template | llm\n",
        "llmOutput=chain.invoke({\"text\":\"Hey I need some veg dishes for my dinner\"})"
      ],
      "metadata": {
        "id": "PqV270a7zK7r"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llmOutput.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9iBeCgwzrBW",
        "outputId": "c1ff9807-102d-4b2d-e5c6-6f6e7a5a2347"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'd be happy to help you with some delicious veg options for dinner.\n",
            "\n",
            "Here are some veg dishes from different restaurants:\n",
            "\n",
            "**From Bistro Bliss**\n",
            "\n",
            "* Stuffed Mushrooms (₹188) - Mushrooms filled with cheese and herbs, baked to perfection.\n",
            "* Margherita Pizza (₹296) - Classic pizza with fresh tomatoes, mozzarella, and basil.\n",
            "* Veg Biryani (₹518) - Aromatic rice dish with mixed vegetables and spices.\n",
            "\n",
            "**From Taste Paradise**\n",
            "\n",
            "* Caprese Salad (₹128) - Sliced tomatoes, mozzarella, and basil with balsamic glaze.\n",
            "* Veggie Wrap (₹501) - Wrap filled with fresh vegetables and a creamy dressing.\n",
            "* Quinoa Salad (₹571) - Quinoa mixed with vegetables and a tangy vinaigrette.\n",
            "\n",
            "**From Savory Delights**\n",
            "\n",
            "* Paneer Butter Masala (₹214) - Paneer cubes cooked in a rich tomato and cream sauce.\n",
            "* Veggie Pizza (₹480) - Pizza loaded with a variety of vegetables.\n",
            "* Greek Salad (₹405) - Tomatoes, cucumbers, olives, and feta cheese with Greek dressing.\n",
            "\n",
            "**From Gastronomy Grove**\n",
            "\n",
            "* Stuffed Mushrooms (₹188) - Mushrooms filled with cheese and herbs, baked to perfection.\n",
            "* Veg Pakoras (₹255) - Vegetables dipped in chickpea flour batter and fried.\n",
            "* Caprese Salad (₹128) - Sliced tomatoes, mozzarella, and basil with balsamic glaze.\n",
            "\n",
            "Which one of these options sounds appealing to you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Llama3 70B is working fine, aligning to our needs"
      ],
      "metadata": {
        "id": "YFm9NNQm1ZgO"
      }
    }
  ]
}